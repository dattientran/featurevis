{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = dei_loss\n",
    "f_eval=base_model\n",
    "x = starting_im\n",
    "mei_act = mei_activation\n",
    "random_dir='random'\n",
    "seed=19\n",
    "target_level = 0.9\n",
    "dev_thre=0.05\n",
    "step_size=4\n",
    "optim_kwargs={'momentum': 0.9}\n",
    "gradient_f=gradient_f\n",
    "transform=None\n",
    "regularization=None\n",
    "optim_name='SGD'\n",
    "optim_kwargs={}\n",
    "i=0\n",
    "\n",
    "# Basic checks\n",
    "if x.dtype != torch.float32:\n",
    "    raise ValueError('x must be of torch.float32 dtype')\n",
    "x = x.detach().clone()  # to avoid changing original\n",
    "x.requires_grad_()\n",
    "\n",
    "# Declare optimizer\n",
    "if optim_name == 'SGD':\n",
    "    optimizer = optim.SGD([x], lr=step_size, **optim_kwargs)\n",
    "elif optim_name == 'Adam':\n",
    "    optimizer = optim.Adam([x], lr=step_size, **optim_kwargs)\n",
    "else:\n",
    "    raise ValueError(\"Expected optim_name to be 'SGD' or 'Adam'\")\n",
    "\n",
    "# Run gradient ascent\n",
    "fevals = []  # to store function evaluations\n",
    "reg_terms = []  # to store regularization function evaluations\n",
    "saved_xs = []  # to store xs (ignored if save_iters is None)\n",
    "\n",
    "torch.manual_seed(4)\n",
    "\n",
    "# Zero gradients\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Transform input\n",
    "transformed_x = x if transform is None else transform(x, iteration=i)\n",
    "\n",
    "# f(x)\n",
    "feval = f(transformed_x)\n",
    "fevals.append(feval.item())\n",
    "\n",
    "# Regularization\n",
    "if regularization is not None:\n",
    "    reg_term = regularization(transformed_x, iteration=i)\n",
    "    reg_terms.append(reg_term.item())\n",
    "else:\n",
    "    reg_term = 0\n",
    "    \n",
    "# Random walk or optimize back to the target activation level\n",
    "dirs = []\n",
    "rand_grad = []\n",
    "if random_dir is None:\n",
    "    (-feval + reg_term).backward()\n",
    "    if gradient_f is not None:\n",
    "        x.grad = gradient_f(x.grad, iteration=i)\n",
    "    x.grad = x.grad / torch.norm(x.grad)\n",
    "\n",
    "elif (f_eval(transformed_x) >= mei_act * (target_level - dev_thre)) and (f_eval(transformed_x) <= mei_act * (target_level + dev_thre)):\n",
    "    direction = torch.randn(x.shape).cuda()\n",
    "    if random_dir == 'random':\n",
    "        x.grad = direction\n",
    "        dirs.append(gradient_f(x.grad, iteration=i) / torch.norm(gradient_f(x.grad, iteration=i)))\n",
    "\n",
    "        if gradient_f is not None:\n",
    "            x.grad = gradient_f(x.grad, iteration=i)\n",
    "        x.grad = x.grad / torch.norm(x.grad)\n",
    "        rand_grad.append(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
